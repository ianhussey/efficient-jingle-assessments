---
title: "Simulate bias and precision of Planned Missing Data Designs + FIML estimation vs complete data vs observed correlations"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r include=FALSE}

# formatting options
# set default chunk options
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

# disable scientific notation
options(scipen = 999) 

```

```{r}

# ------------------------------------------------------------
# Planned-Missing Designs: 2-Factor Simulation Scaffold (R)
# Structure:
#  1) Generate Data
#  2) Analyze
#  3) Scale
#  4) Summarize
#  5) Input Box
# ------------------------------------------------------------

# ---- Libraries ----
library(tidyr)
library(dplyr)
library(purrr)
library(furrr)
library(tibble)
library(rlang)
library(ggplot2)
library(broom)
library(lavaan)
library(mice)
library(glue)
library(janitor)
# Optional: alerts
# library(beepr)

# ---- Reproducibility ----
rm(list = ls())
set.seed(42)
# future::plan(multisession)  # optional for parallel

##############################################
# ---- 1) Data Generator (SEM two-factor) ----
##############################################

# 1.1) Build a population model string (two correlated factors)
make_pop_model <- function(k = 8, lambda = 0.7, rho = 0.50) {
  x_items <- paste0("x", seq_len(k))
  y_items <- paste0("y", seq_len(k))
  glue::glue('
    F1 =~ {lambda}*{x_items[1]} + {lambda}*{paste(x_items[-1], collapse=" + ")}
    F2 =~ {lambda}*{y_items[1]} + {lambda}*{paste(y_items[-1], collapse=" + ")}
    F1 ~~ 1*F1
    F2 ~~ 1*F2
    F1 ~~ {rho}*F2
  ')
}

# 1.2) Generate observed item data from the population model
generate_data <- function(n, k = 8, lambda = 0.7, rho = 0.50, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  pop_model <- make_pop_model(k = k, lambda = lambda, rho = rho)
  df_full <- lavaan::simulateData(
    model        = pop_model,
    sample.nobs  = n,
    standardized = TRUE
  ) |>
    tibble::as_tibble()
  attr(df_full, "truth") <- list(k = k, lambda = lambda, rho = rho, n = n)
  df_full
}


##############################################################
# ---- 1b) Planned-Missing Mechanisms (block-MCAR per scale) ----
##############################################################

# 1b.1) Helper: exact block masking on a numeric matrix
#   mat  : numeric matrix (n x k)
#   prop : proportion of missing values (0..1) per row
#   rule : rounding rule for m (round/floor/ceiling)
#   seed : optional seed for reproducible column selection
mask_block_exact <- function(mat, prop, rule = c("round", "floor", "ceiling"), seed = NULL) {
  stopifnot(is.matrix(mat))
  rule <- match.arg(rule)
  if (!is.null(seed)) set.seed(seed)
  
  n <- nrow(mat); k <- ncol(mat)
  if (k == 0L || n == 0L) return(mat)
  
  # determine the number of missing values (m) per row according to the rounding rule
  m <- switch(rule,
              round   = as.integer(round(prop * k)),
              floor   = as.integer(floor(prop * k)),
              ceiling = as.integer(ceiling(prop * k))
  )
  m <- max(0L, min(k, m)) # ensure m is not smaller than 0 and not larger than k
  
  if (m == 0L) return(mat) # no missing values → return the matrix unchanged
  if (m == k) return(matrix(NA_real_, nrow = n, ncol = k, dimnames = dimnames(mat)))
  # if all items should be missing (m = k), return a matrix filled with NAs
  
  # define which cells should be missing
  miss <- matrix(FALSE, nrow = n, ncol = k) # create logical mask (all FALSE initially)
  idx_list <- replicate(
    n,
    sample.int(k, size = m, replace = FALSE), # draw m unique column indices (1..k) per row
    simplify = FALSE
  )
  
  # for each row i, mark the selected columns as TRUE in the missingness mask
  for (i in seq_len(n)) miss[i, idx_list[[i]]] <- TRUE
  
  # assign NA to all positions marked as TRUE in the mask → installs NAs into input matrix
  mat[miss] <- NA_real_
  mat
}

# 1b.2) Orchestrator: apply block-MCAR within x/y scales (by column sets)
#   Applies mask_block_exact() separately to x- and y-item blocks.
impose_missing_by_scale <- function(df, k = 8, prop = 0.50, seed = NULL,
                                    rule = c("round", "floor", "ceiling"),
                                    cols_x = NULL, cols_y = NULL) {
  stopifnot(is.data.frame(df), k >= 1, prop >= 0, prop <= 1)
  rule <- match.arg(rule)
  
  # determine which columns belong to the x and y blocks
  if (is.null(cols_x)) cols_x <- paste0("x", seq_len(k))
  if (is.null(cols_y)) cols_y <- paste0("y", seq_len(k))
  
  out <- df # working copy of original simulated df
  
  # X block: select → to matrix → mask → write back
  out[, cols_x] <- mask_block_exact(
    as.matrix(df[, cols_x, drop = FALSE]),
    prop, rule, seed
  )
  
  # Y block: select → to matrix → mask → write back
  out[, cols_y] <- mask_block_exact(
    as.matrix(df[, cols_y, drop = FALSE]),
    prop, rule, seed
  )
  
  out
}



#################################
# ---- 2) Analyzer (modular) ----
#################################

# 2.1) Build item names from k
make_item_names <- function(k) {
  list(
    x = paste0("x", seq_len(k)),
    y = paste0("y", seq_len(k))
  )
}

# 2.2) Build a two-factor measurement model string (F1 loads on x, F2 on y)
build_meas_model <- function(x_items, y_items) {
  glue::glue('
    F1 =~ {paste(x_items, collapse = " + ")} # creates plain text F1 =~ x1 + x2 + ...
    F2 =~ {paste(y_items, collapse = " + ")}
  ')
}

# 2.3) Fit CFA (returns lavaan fit)
fit_cfa <- function(data, meas_model, use_fiml = TRUE) {
  lavaan::sem(
    model         = meas_model,
    data          = data,
    meanstructure = TRUE,
    estimator     = "ML",
    std.lv        = TRUE,   # latent vars var=1 ⇒ F1~~F2 is a correlation
    missing       = if (use_fiml) "fiml" else "listwise",
    warn          = FALSE
  )
}

# 2.4) Extract the latent correlation row (F1~~F2); if missing, return NA row
extract_latent_corr <- function(fit) {
  pe_lat <- lavaan::parameterEstimates(fit, standardized = FALSE, ci = TRUE) |>
    dplyr::filter(lhs == "F1", op == "~~", rhs == "F2") |>
    dplyr::transmute(
      measure   = "latent",
      est       = est,
      se        = se,
      lwr       = ci.lower,
      upr       = ci.upper,
      converged = lavaan::lavInspect(fit, "converged")
    )
  if (nrow(pe_lat) == 0L) {
    tibble::tibble(
      measure   = "latent",
      est       = NA_real_,
      se        = NA_real_,
      lwr       = NA_real_,
      upr       = NA_real_,
      converged = FALSE
    )
  } else pe_lat
}

# (micro-helper) Row means for a given set of columns, handling NA (ignoring NA)
row_means_safe <- function(data, cols) {
  rowMeans(data[, cols, drop = FALSE], na.rm = TRUE)
}

# 2.5) Observed mean-scale correlation (rowMeans of x/y + cor.test)
observed_mean_corr <- function(data, x_items, y_items) {
  x_mean <- row_means_safe(data, x_items) # using micro helper, ignoring NA
  y_mean <- row_means_safe(data, y_items)
  cc <- stats::complete.cases(x_mean, y_mean) # returns logical vector marking rows with no missing values
  
  if (sum(cc) >= 4L) { # minimum number of 4 complete pairs
    ct <- stats::cor.test(x_mean[cc], y_mean[cc])  # Pearson r + Confidence Interval
    tibble::tibble(
      measure   = "observed",
      est       = unname(ct$estimate[[1]]), # observed correlation r
      se        = 1 / sqrt(sum(cc) - 3),    # approximate standard error or r using Fisher's z transformation
      lwr       = unname(ct$conf.int[1]),   # upper bound of CI
      upr       = unname(ct$conf.int[2]),   # lower bound of CI
      converged = TRUE # if cor.test() computed, converged set to TRUE
    )
  } else {
    tibble::tibble(
      measure   = "observed",
      est       = NA_real_,
      se        = NA_real_,
      lwr       = NA_real_,
      upr       = NA_real_,
      converged = FALSE # if fewer than 4 complete pairs exist, correlation estimation is skipped and converged = FALSE
    )
  }
}


# 2.6) ANALYZE (combines functions defined above 1–5)
analyze <- function(data, k = 8, use_fiml = TRUE) {
  nm <- make_item_names(k)                      # build item name lists: x1..xk and y1..yk
  meas_model <- build_meas_model(nm$x, nm$y)    # create measurement model string: "F1 =~ x1 + ...; F2 =~ y1 + ..."
  fit <- fit_cfa(data, meas_model, use_fiml)    # fit CFA model using lavaan (ML + FIML or listwise)
  dplyr::bind_rows(
    extract_latent_corr(fit),                   # extract latent factor correlation (F1~~F2)
    observed_mean_corr(data, nm$x, nm$y)        # compute observed correlation between mean scale scores per person
  )
}


# 2.7) COMPARE full vs. missing data analyses
# Runs the analyze() function on:
#   1) the complete dataset (listwise deletion), and
#   2) the planned-missing dataset (using FIML),
# then combines both results and adds the true population correlation (rho).
analyze_both <- function(df_full, df_miss, k, rho) {
  
  # analyze the full (complete) data
  r_full <- analyze(df_full, k = k, use_fiml = FALSE) |>
    dplyr::mutate(dataset = "full")
  
  # analyze the dataset with planned missingness (FIML handles NAs)
  r_miss <- analyze(df_miss, k = k, use_fiml = TRUE) |>
    dplyr::mutate(dataset = "missing_FIML")
  
  # combine both results and attach the true latent correlation (rho)
  dplyr::bind_rows(r_full, r_miss) |>
    dplyr::mutate(r_true = rho)
}



#########################################
# ---- 3) Scaling (Runners) ----
#########################################

# 3.1) One replication: generate -> impose planned missing -> analyze
run_one <- function(n, k, lambda, rho, miss_prop, seed = NULL) {
  
  df_full <- generate_data(n = n, k = k, lambda = lambda, rho = rho, seed = seed)
  
  df_miss <- impose_missing_by_scale(df_full, k = k, prop = miss_prop, seed = seed)
  
  analyze_both(df_full, df_miss, k = k, rho = rho)  # returns 2 rows (latent & observed)
}

# 3.2) R replications for one condition; attach replication + condition metadata
run_condition <- function(R, n, k, lambda, rho, miss_prop, cond_id = 1, seed = NULL) {
  purrr::map_dfr(seq_len(R), function(r) {
    # optional deterministic seed per replication (uncomment if desired)
    # seed_r <- if (!is.null(seed)) as.integer(seed + r + cond_id*1e6) else NULL
    seed_r <- NULL
    run_one(n, k, lambda, rho, miss_prop, seed = seed_r) |>
      dplyr::mutate(
        rep       = r,
        cond_id   = cond_id,
        n         = n,
        k         = k,
        lambda    = lambda,
        rho       = rho,
        miss_prop = miss_prop
      )
  })
}

# 3.3) Run a whole parameter grid (tibble with columns: R,n,k,lambda,rho,miss_prop)
#     Parallel-ready (set plan() outside); returns UNNESTED long results.
run_grid <- function(grid, .progress = TRUE, seed = TRUE) {
  # expect columns R, n, k, lambda, rho, miss_prop
  res <- furrr::future_pmap_dfr(
    .l = list(grid$R, grid$n, grid$k, grid$lambda, grid$rho, grid$miss_prop, seq_len(nrow(grid))),
    .f = run_condition,
    .progress = .progress,
    .options  = furrr::furrr_options(seed = seed)
  )
  res
}


########################
# ---- 4) Summarize ----
########################

# 4.1) Summarize within (dataset x measure) while respecting upstream condition groups
summarise_sim <- function(res) {
  res |>
    dplyr::group_by(dataset, measure, .add = TRUE) |>
    dplyr::summarise(
      n_iter         = sum(!is.na(est)),
      converged_prop = mean(converged, na.rm = TRUE),
      mean_est       = mean(est, na.rm = TRUE),
      bias           = mean(est - r_true, na.rm = TRUE),
      rmse           = sqrt(mean((est - r_true)^2, na.rm = TRUE)),
      sd_est         = stats::sd(est, na.rm = TRUE),
      mean_se        = mean(se, na.rm = TRUE),
      mean_ci_width  = mean(upr - lwr, na.rm = TRUE),
      coverage_95    = mean(lwr <= r_true & upr >= r_true, na.rm = TRUE),
      mcse_bias      = ifelse(n_iter > 0, stats::sd(est - r_true, na.rm = TRUE) / sqrt(n_iter), NA_real_),
      mcse_cov95     = ifelse(n_iter > 0, sqrt(coverage_95 * (1 - coverage_95) / n_iter), NA_real_),
      .groups = "drop"
    )
}

# 4.2) Convenience: summarize by condition columns automatically
summarise_grid <- function(res_long) {
  res_long |>
    dplyr::group_by(cond_id, n, k, lambda, rho, miss_prop) |>
    summarise_sim()
}


#############################################
# ---- 5) Entry Points (user inputs) ----
#############################################

# A) Single condition -----------------------------------------------------
# Define inputs:
n         <- 300
k         <- 8
lambda    <- 0.7
rho       <- 0.5
miss_prop <- 0.5
R         <- 10
seed      <- 123

set.seed(seed)
results_one <- run_condition(R, n, k, lambda, rho, miss_prop, cond_id = 1)
summary_one <- results_one |> summarise_sim()
print(summary_one)
View(summary_one)

# B) Grid run -------------------------------------------------------------
# Define a grid (edit as needed)
experiment_parameters <- tidyr::expand_grid(
  n         = c(100, 300, 500),
  k         = c(4, 10, 20),
  lambda    = c(0.5, 0.7),
  rho       = c(0.2, 0.5, 0.8),
  miss_prop = c(0.3, 0.5, 0.7),
  R         = 1000
)

# Parallel plan is set outside (e.g., plan(multisession)); then:
grid_results <- run_grid(experiment_parameters, .progress = TRUE, seed = TRUE)
simulation_summary <- summarise_grid(grid_results)
print(simulation_summary)

```

